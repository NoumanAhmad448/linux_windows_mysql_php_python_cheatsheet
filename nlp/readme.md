## Research Keywords | LLM keywords | Coding Challenges Keywords
```Always keep it at the top``` </br>
For reference [https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=label:llm](https://scholar.google.com/citations?view_op=search_authors&hl=en&mauthors=label:llm)
1. label:language_modelling
2. label:llm_agent
3. label:evaluation
4. label:NLP
5. label:code_generation
6. 
</br>```Always keep it at the top```


## NLP LLM Prompt Training Terminologies

|    |    |    ||
|------------|------------|------------|------------|
| *Finetuning method* | lora | freeze | freeze |
| *qlora* | 4 | 8 |
| *Rope Scalling* | Linear | Dynamic |
| *Booster* | flashattn2 | unsloth |linger |
| *Stage* | Supervised Fine Training |
| *Compute Type* | bf16 | fp16 |


## After training model to run model 
1. https://github.com/sgl-project/sglang
2. https://github.com/vllm-project/vllm%E3%80%90api
3. 


## During training of models
1. https://github.com/microsoft/DeepSpeed
2. https://github.com/unslothai/unsloth

## web ui developer
1. https://github.com/gradio-app/gradio
2. To develop command line apps
   ```
   setuptools and find_packages
   ```
3. 

## Search Models 
1. https://www.modelscope.cn/
2. https://modelers.cn/models
3. https://huggingface.co
4. 


# Terminologies
Large langauge models,NLP,code generation,LLM benchmarks,Eval benchmarks,Code Summarization,
In-Context Learning

## Companies
- Hugging face
- microsoft
- OPENAI
- Google
- meta


## Technical Terms
- Word2Vec
- GloVe
- Fine-tuning
- infilling
- Zero-shot learning is a prompt that has no examples
- One-shot has one example, and
- few-shot has more than one example.
- Codex: LLM by OPENAI
- Tokenization
- Stemming
- POS(part of speech)
- lemmatization

## NLP:
->TF_IDF,Bi-Gram and Tri-Gram,Bag of Words BOW,OneHotEncoding Text


## Hugging Face:
Text classification	assign a label to a given sequence of text	NLP	pipeline(task=“sentiment-analysis”)
Text generation	generate text given a prompt	NLP	pipeline(task=“text-generation”)
Summarization	generate a summary of a sequence of text or document	NLP	pipeline(task=“summarization”)

## python libraries
- Reading words in the email Enron_Emails_Dataset_Processed kmcluster
https://towardsdatascience.com/how-i-used-machine-learning-to-classify-emails-and-turn-them-into-insights-efed37c1e66
-  training eron emails over multiple models  KNeighborsClassifier,DecisionTreeClassifier,LogisticRegression
https://github.com/soliverc/Enron-Ham-Spam-Email-FIlter/blob/master/Enron%20HamSpam.ipynb
-  Network Construction python library
https://github.com/RutujChheda/Enron_Emails_Dataset_Processed/blob/main/Enron_Data_Normalization.ipynb
https://www.kaggle.com/code/lakshmi25npathi/sentiment-analysis-of-imdb-movie-reviews/notebook
https://www.kaggle.com/code/mfaaris/content-based-and-tensorflow-recommender-system/notebook
